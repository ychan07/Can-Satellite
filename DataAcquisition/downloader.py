import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

# âœ… í˜„ì¬ Python ìŠ¤í¬ë¦½íŠ¸ê°€ ìˆëŠ” ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ ì €ì¥ ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(SCRIPT_DIR, "mangaHI_csv_files_DR3")

# ğŸŒ CSV íŒŒì¼ì´ ìˆëŠ” ì›¹ì‚¬ì´íŠ¸ ì£¼ì†Œ
BASE_URL = "https://www.gb.nrao.edu/GbtLegacyArchive/HI-MANGA/DR3/spectra/gbt/csv/"
MAX_THREADS = 50  # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ìˆ˜ (ì¸í„°ë„· í™˜ê²½ì— ë”°ë¼ ì¡°ì •)

# ğŸ“ ì €ì¥ í´ë” ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ğŸŒ HTMLì—ì„œ CSV ë§í¬ ê°€ì ¸ì˜¤ê¸°
response = requests.get(BASE_URL)
soup = BeautifulSoup(response.text, "html.parser")
csv_links = [
    urljoin(BASE_URL, a['href']) for a in soup.find_all('a', href=True)
    if a['href'].endswith(".csv")
]

print(f"[INFO] ì´ {len(csv_links)}ê°œì˜ CSV íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")

# â¬‡ï¸ ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
def download_csv(url):
    file_name = os.path.join(OUTPUT_DIR, os.path.basename(url))
    try:
        r = requests.get(url, timeout=20)
        r.raise_for_status()
        with open(file_name, 'wb') as f:
            f.write(r.content)
        return f"[OK] {file_name}"
    except Exception as e:
        return f"[FAIL] {file_name} - {e}"

# ğŸ” ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
    future_to_url = {executor.submit(download_csv, url): url for url in csv_links}
    for i, future in enumerate(as_completed(future_to_url), 1):
        result = future.result()
        print(f"[{i}/{len(csv_links)}] {result}")

print("\nâœ… ë³‘ë ¬ ë‹¤ìš´ë¡œë“œê°€ ëª¨ë‘ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
